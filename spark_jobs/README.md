# Spark Jobs for Retail Inventory Management

This directory contains Apache Spark applications for real-time streaming, batch processing, and machine learning in the retail inventory management system.

## Overview

The Spark applications are designed to process data from Kafka streams, perform real-time analytics, train machine learning models, and generate business intelligence reports.

## Applications

### 1. Sales Streaming (`sales_streaming.py`)
- **Purpose**: Real-time processing of sales events from Kafka
- **Features**:
  - Consumes sales events from `sales_events` Kafka topic
  - Calculates real-time metrics (revenue, transaction counts, customer counts)
  - Detects anomalies in sales patterns
  - Aggregates data in 5-minute windows with 1-minute slides
  - Outputs to console (can be configured for ClickHouse)

### 2. Inventory Streaming (`inventory_streaming.py`)
- **Purpose**: Real-time monitoring of inventory levels
- **Features**:
  - Consumes inventory events from `inventory_events` Kafka topic
  - Tracks stock levels and detects low/out-of-stock situations
  - Generates stock alerts with severity levels
  - Calculates inventory turnover metrics
  - Uses 10-minute windows with 2-minute slides

### 3. ML Training (`ml_training.py`)
- **Purpose**: Train machine learning models for demand forecasting and stock optimization
- **Features**:
  - Loads historical data from ClickHouse
  - Trains Random Forest model for demand forecasting
  - Trains Gradient Boosted Trees for stock optimization
  - Evaluates model performance using RMSE
  - Saves trained models for deployment

### 4. Batch Processing (`batch_processing.py`)
- **Purpose**: Daily batch processing and report generation
- **Features**:
  - Calculates daily KPIs and business metrics
  - Generates product and store performance rankings
  - Creates comprehensive daily business reports
  - Updates materialized views (when connected to ClickHouse)

## Usage

### Prerequisites
- Running Spark cluster (master and workers)
- Kafka cluster with topics created
- ClickHouse database with schema initialized
- Data generator running to produce sample data

### Job Submission

Use the provided submission script to submit jobs to the Spark cluster:

```bash
# Make the script executable
chmod +x submit_jobs.sh

# Submit individual jobs
./submit_jobs.sh sales        # Submit sales streaming
./submit_jobs.sh inventory    # Submit inventory streaming
./submit_jobs.sh ml           # Submit ML training
./submit_jobs.sh batch        # Submit batch processing

# Submit all streaming jobs
./submit_jobs.sh all

# Check job status
./submit_jobs.sh status

# Clean checkpoint data
./submit_jobs.sh clean
```

### Manual Submission

You can also submit jobs manually using `spark-submit`:

```bash
# Submit from within Spark container
docker exec -it spark-master bash

# Submit sales streaming
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,com.clickhouse:clickhouse-jdbc:0.4.6" \
  /opt/spark/jobs/sales_streaming.py
```

## Configuration

### Spark Configuration
- **Master URL**: `spark://spark-master:7077`
- **Deploy Mode**: Client
- **Memory**: 1-2GB per executor/driver
- **Cores**: 1-2 cores per executor

### Kafka Configuration
- **Bootstrap Servers**: `kafka:9092`
- **Topics**: `sales_events`, `inventory_events`
- **Consumer Group**: `{application}-group`

### ClickHouse Configuration
- **URL**: `jdbc:clickhouse://clickhouse:8123/default`
- **Driver**: `com.clickhouse.jdbc.ClickHouseDriver`
- **User**: `default`
- **Password**: `clickhouse`

## Monitoring

### Spark UI
- Access Spark Master UI at: `http://localhost:8082`
- View running applications, executors, and job status

### Kafka UI
- Access Kafka UI at: `http://localhost:8080`
- Monitor topics, consumer groups, and message flow

### Logs
- Application logs are written to Spark logs
- Checkpoint data stored in `/tmp/checkpoint/`

## Data Flow

1. **Data Generation** → Fake data generated by data generator
2. **Kafka Topics** → Events published to `sales_events` and `inventory_events`
3. **Spark Streaming** → Real-time processing and analytics
4. **ClickHouse** → Processed data stored for querying
5. **ML Models** → Batch training on historical data
6. **Reports** → Daily batch processing generates business insights

## Development

### Adding New Jobs
1. Create new Python file in this directory
2. Follow the existing structure with proper logging and error handling
3. Update `submit_jobs.sh` with new submission command
4. Test locally before deployment

### Dependencies
- PySpark 3.4.1
- Kafka Spark Connector
- ClickHouse JDBC Driver
- Required Python packages (see requirements.txt)

### Testing
- Use sample data from data generator
- Monitor console output for processing results
- Check Spark UI for job status and metrics
- Validate data in ClickHouse after processing

## Troubleshooting

### Common Issues
1. **Spark Master Not Available**: Ensure Spark containers are running
2. **Kafka Connection Issues**: Check Kafka broker connectivity
3. **ClickHouse Connection**: Verify database is accessible
4. **Checkpoint Errors**: Clean checkpoint directory if corrupted

### Logs and Debugging
- Check Spark driver logs for application errors
- Monitor Kafka consumer lag in Kafka UI
- Validate data schemas match between producers and consumers
- Use debug logging level for detailed troubleshooting

## Performance Tuning

### Streaming Applications
- Adjust `maxRatePerPartition` based on data volume
- Tune window sizes and watermarks
- Optimize checkpoint intervals
- Monitor backpressure indicators

### Batch Applications
- Configure appropriate memory settings
- Use adaptive query execution
- Optimize data partitioning
- Leverage caching for repeated operations

### ML Applications
- Balance model complexity with training time
- Use cross-validation for hyperparameter tuning
- Monitor memory usage during training
- Consider model serialization for deployment